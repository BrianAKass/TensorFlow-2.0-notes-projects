{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14-01 Artificial Neural Networks Theory.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX0iNgFBdp01",
        "colab_type": "text"
      },
      "source": [
        "## Things we will learn:\n",
        "\n",
        " * The Neuron\n",
        " \n",
        " * Activation Function\n",
        " \n",
        " * How do Neural Networks work? (example)\n",
        " \n",
        " * Gradient Descent\n",
        " \n",
        " * Stochastic Gradient Descent\n",
        " \n",
        " * Backpropagation\n",
        " \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJkSxzvBek5W",
        "colab_type": "text"
      },
      "source": [
        "## The Neuron\n",
        "\n",
        "![alt text](https://i.imgur.com/yBbaK2V.png)\n",
        "\n",
        "* dendrites act like a receiver\n",
        "\n",
        "* axons act as the transmitter\n",
        "\n",
        "* By itself, useless\n",
        "\n",
        "* In groups, can do a lot \n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/6I6W04h.png)\n",
        "\n",
        "* visual example signal being passed is a synapse.\n",
        "\n",
        "## How do we replicate that in ML?\n",
        "\n",
        "![alt text](https://i.imgur.com/nIeHdO5.png)\n",
        "\n",
        "* See the family resemblence? \n",
        "  \n",
        "![alt text](https://i.imgur.com/UkJW2HV.png)\n",
        "  \n",
        "![alt text](https://i.imgur.com/w1TJJ7b.png)\n",
        "  \n",
        "  \n",
        "## More reading:\n",
        "\n",
        "**Efficient BackProp**\n",
        "By Yann LeChun et al. (1998)\n",
        "\n",
        "http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
        "\n",
        "\n",
        "### Output value :\n",
        "\n",
        "Can be:\n",
        "  * Continuous (price)\n",
        "  \n",
        "  * Binary (will exit yes/no)\n",
        "  \n",
        "  * Categorical so it might look like bellow. \n",
        "\n",
        "  \n",
        "![alt text](https://i.imgur.com/ClOT5kp.png)\n",
        "  \n",
        "  for both the input and output we have the same single observation. \n",
        "  \n",
        "  \n",
        "###The \"Synapses\"\n",
        "\n",
        "They are the wieghts in our inputs. \n",
        "\n",
        "\n",
        "  \n",
        "![alt text](https://i.imgur.com/1KnXRNh.png)\n",
        "\n",
        "## So what's going on in the neuron?\n",
        "\n",
        "* first step is it's adding all the values \n",
        "\n",
        "![alt text](https://i.imgur.com/MWqLQvU.png)\n",
        "\n",
        "* then an activation function\n",
        "\n",
        "![alt text](https://i.imgur.com/qI5rqvY.png)\n",
        "\n",
        "* finally it passes on to the next neuron down the line \n",
        "\n",
        "![alt text](https://i.imgur.com/s29kDkg.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHKaNpOiuSKy",
        "colab_type": "text"
      },
      "source": [
        "# The Activation Function (aka the third step.)\n",
        "\n",
        "## Four Primary Types: \n",
        "\n",
        "### Threshold Function\n",
        "\n",
        "![alt text](https://i.imgur.com/z129rEm.png)\n",
        "\n",
        "Good for binary true false yes no cases\n",
        "\n",
        "### Sigmoid Function\n",
        "\n",
        "![alt text](https://i.imgur.com/Tm68zmd.png)\n",
        "\n",
        "* Used for logistic regression\n",
        " * smooth better used for probability problems. \n",
        " \n",
        " ### Rectifier Function\n",
        "\n",
        "![alt text](https://i.imgur.com/EQzrc4s.png)\n",
        "\n",
        "* Most commonly used\n",
        "* for values expected to increase \n",
        "\n",
        "### Hyperbolic Tangent (tanh)\n",
        "\n",
        "![alt text](https://i.imgur.com/RjLtRkM.png)\n",
        "\n",
        "* Like sigmoid, but can go bellow 0\n",
        "\n",
        "Additional Reading:\n",
        "\n",
        "Deep Sparse Rectifier Neural Networks\n",
        "\n",
        "by Xavier Glorot et al. (2011)\n",
        "\n",
        "http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXCRJUkyok1x",
        "colab_type": "text"
      },
      "source": [
        "Quick exercise. \n",
        "\n",
        "Assuming the DV (Depndent variable is binary) Which threshold would i choose? Threshold function. possibly the sigmoid if there is need to estimate probability. \n",
        "\n",
        "![alt text](https://i.imgur.com/Q3vdXsY.png)\n",
        "\n",
        "usually in the hidden layer we will apply the rectifier function followed by whatever required function for output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hubp8C0BxRfe",
        "colab_type": "text"
      },
      "source": [
        "## How do NNs Work?\n",
        "\n",
        "Lots of images ahead in this. Just warning everyone now. We are going to estimate property values. \n",
        "\n",
        "\n",
        "In it's basic form the neural network has it's input and output layer. \n",
        "\n",
        "![alt text](https://i.imgur.com/rnmunHl.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZQdHBesydv1",
        "colab_type": "text"
      },
      "source": [
        "In this primitive form the input would be weighed by it's 'synapses' and simply calculated from there. \n",
        "\n",
        "![alt text](https://i.imgur.com/37zLuYB.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYix4t6-y_ME",
        "colab_type": "text"
      },
      "source": [
        "This of course doesn't mean we will get accurate results, but it's imporessive we get an output at all on this primitive level. So from there of course we add the hidden layers. From there we will break it down further. \n",
        "\n",
        "![alt text](https://i.imgur.com/dEKsTbf.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcZPcEiKznHg",
        "colab_type": "text"
      },
      "source": [
        "So to start all 4 variables on the left. and we start on the fist neuron on the top of the hidden layer. All of which will have weights. Some will have non zero values others will have a zero value. remember not all data has as much importance or correlation as other data has. \n",
        "\n",
        "![alt text](https://i.imgur.com/3bED9YP.png)\n",
        "\n",
        "\n",
        "### For the sake of simplicity in the next set of images zero values will not have their dotted lines there for the sake of simplifying our images and removing diversions of where our focus should be. \n",
        "\n",
        "![alt text](https://i.imgur.com/e34g6k9.png)\n",
        "\n",
        "\n",
        "Got it? Good. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhXs9N9qzqez",
        "colab_type": "text"
      },
      "source": [
        "So now we will connect to another neuron. Why are these connecting? \n",
        "\n",
        "![alt text](https://i.imgur.com/eIKzu1P.png)\n",
        "\n",
        "Looks like we have another set of coorelations found. \n",
        "\n",
        "So now lets look at the bottom one. \n",
        "\n",
        "![alt text](https://i.imgur.com/CiVVEQP.png)\n",
        "\n",
        "Looks like age itself is a significant here in it's own right. Looks like a good place for a rectifier function could be applied here. \n",
        "\n",
        "![alt text](https://i.imgur.com/4zQ5jNV.png)\n",
        "\n",
        "So the inputs in short are finding connections based on correlations they apply to each neuron on the hidden layer. These can be applied and suited for our given task. From there we add the weigts and dropouts to iron out things, and it goes to the output from there. \n",
        "\n",
        "![alt text](https://i.imgur.com/WZTkcIJ.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3V0HcOfzrGf",
        "colab_type": "text"
      },
      "source": [
        "# How Do NNs Learn?\n",
        "\n",
        "Here is our perceptron or single layer neural network. \n",
        "\n",
        "![alt text](https://i.imgur.com/QFT6Prp.png)\n",
        "\n",
        "on the left the input values run though the neural network, the activation function is applied and we get our output and it's plotted on the chart.\n",
        "\n",
        "From there we compare to the actual value we want. We from there calculate the cost function which is basically a glorified difference between the toutput value and acutal value whether positive or negative. to make sure the machine fits to the training data. The closer it is to the actual value the better off we are to yielding correct results. \n",
        "\n",
        "![alt text](https://i.imgur.com/D4POcEg.png)\n",
        "\n",
        "Once compared we feed the info to our neural network and make corrections updating our weights in the given model. \n",
        "\n",
        "![alt text](https://i.imgur.com/1nGBC8a.png)\n",
        "\n",
        "now in this example we are dealing with just a single row. There will be other rows to deal with. For the sake of simplicty we are only dealing with one row. Now lets see what running a bunch of epochs looks like.\n",
        "\n",
        "![alt text](https://i.imgur.com/HlHJSfR.png)\n",
        "\n",
        "from there we check the cost function and update the weights \n",
        "\n",
        "![alt text](https://i.imgur.com/NqLSy9I.png)\n",
        "\n",
        "All rows share the weights. \n",
        "\n",
        "**Additional reading :**\n",
        "\n",
        "A list of cost functions used in neural networks, alongside applications\n",
        "\n",
        "link:\n",
        "\n",
        "https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyN_PeJvzsKk",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Descent\n",
        "\n",
        "\"So how are those weights adjusted in our neural network?\" Good question.\n",
        "\n",
        "So if we wanted to brute force things we'd keep plotting until we hit the lowest point possible on our input values. But this tanks our efficiency and slows us down significantly the more complexity we add to our neural network. This is the curse of dimensionality.\n",
        "\n",
        "So instead we will use gradient descent. We wrote out a lot of this in our ealier machine learning notes so we will refer to those notes. \n",
        "\n",
        "# Sotchastic Gradient Descent\n",
        "\n",
        "So what if our function does not have a convex shape to itself? \n",
        "\n",
        "![alt text](https://i.imgur.com/s2u7rKU.png)\n",
        "\n",
        "Using a typical gradient descent method, we might even miss the closest fits to our data. \n",
        "\n",
        "normal gradient descent takes all our rows and plugs them into our neural network, calculate cost, and finally adjust our weights. \n",
        "\n",
        "Stochastic gradient descent we take the rows one at a time adjusting our weights as we go along. Lather, rinse, and repeat\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/8A9tBqI.png)\n",
        "\n",
        "One other advantage is stochastic can be faster than regular gradient descent. Doing the ccalulations one at a tmie is usually faster than all at once. \n",
        "\n",
        "A happy medium between the two is mini batch gradient descent. So basically gradient descent in a bite size friendly form. Keep that one in mind. \n",
        "\n",
        "**additional reading** \n",
        "\n",
        "A neural network in 13 lines of python ( part 2 Gradient Descent) \n",
        "\n",
        "Andrew Trask (2015)\n",
        "\n",
        "Link \n",
        "\n",
        "https://iamtrask.github.io/2015/07/27/python-network-part2/\n",
        "\n",
        "\n",
        "Neural Networks and Deep Learning \n",
        "\n",
        "Michael Nielson (2015)\n",
        "\n",
        "link \n",
        "\n",
        "http://neuralnetworksanddeeplearning.com/chap2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWStsJqrzsgN",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "Forward propagation is as we know moving forward in out neural network. From there we calculate errors and go back to change the weights This is the Backpropagation. In short  its yet another advanced algorithm which allows us to adjust all the weights at the same time. \n",
        "\n",
        "The key thing to remember is durring the process of backpropagation because of the way the algorithm is structured, you are able to adjust all the weights at the same time so you know which part of error each of your weights in your neural network is responsible for.\n",
        "\n",
        "The math behind this can be found here. \n",
        "\n",
        "Neual networks and deep learning Michael Nielson (2015)\n",
        "http://neuralnetworksanddeeplearning.com/chap2\n",
        "\n",
        "To wrap things up: \n",
        "\n",
        "### Step 1:\n",
        "Randomly initialize the weights to small numbers close to 0 (but not 0)\n",
        "\n",
        "### Step 2: \n",
        "Input the first observation of your dataset in the input layer, each feature in one input mode. \n",
        "\n",
        "### Step 3: \n",
        "Forward-Propagation: from left to right, neurons are activated in a way that the impact of each neuron's activation is limited by weights. propagate the activations until getting the predicted result of y.  \n",
        "\n",
        "### Step 4:\n",
        "Compare the predicted results to the actual result. Measure the generated error. \n",
        "\n",
        "### Step 5:\n",
        "Back-Propagation: from right to left. the error is back-propagated. Update the weights according to how much they are responsible for the error. the learning rate decides by how much we update the weights. \n",
        "\n",
        "### Step 6:\n",
        "repeat steps 1-5 and update weights after each observation (reinforcement learning) or: repeat Steps 1 to 5 but update weiights only after a batch of operations (batch learning)\n",
        "\n",
        "### Step 7:\n",
        "When the whole training set passed through the ANN, that makes an epoch. Redo more Epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwGLo6DDzs7n",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L10WryIztSy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPFzzEjGztsX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}