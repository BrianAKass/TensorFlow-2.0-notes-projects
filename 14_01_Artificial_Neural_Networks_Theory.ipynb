{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14-01 Artificial Neural Networks Theory.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX0iNgFBdp01",
        "colab_type": "text"
      },
      "source": [
        "## Things we will learn:\n",
        "\n",
        " * The Neuron\n",
        " \n",
        " * Activation Function\n",
        " \n",
        " * How do Neural Networks work? (example)\n",
        " \n",
        " * Gradient Descent\n",
        " \n",
        " * Stochastic Gradient Descent\n",
        " \n",
        " * Backpropagation\n",
        " \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJkSxzvBek5W",
        "colab_type": "text"
      },
      "source": [
        "## The Neuron\n",
        "\n",
        "![alt text](https://i.imgur.com/yBbaK2V.png)\n",
        "\n",
        "* dendrites act like a receiver\n",
        "\n",
        "* axons act as the transmitter\n",
        "\n",
        "* By itself, useless\n",
        "\n",
        "* In groups, can do a lot \n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/6I6W04h.png)\n",
        "\n",
        "* visual example signal being passed is a synapse.\n",
        "\n",
        "## How do we replicate that in ML?\n",
        "\n",
        "![alt text](https://i.imgur.com/nIeHdO5.png)\n",
        "\n",
        "* See the family resemblence? \n",
        "  \n",
        "![alt text](https://i.imgur.com/UkJW2HV.png)\n",
        "  \n",
        "![alt text](https://i.imgur.com/w1TJJ7b.png)\n",
        "  \n",
        "  \n",
        "## More reading:\n",
        "\n",
        "**Efficient BackProp**\n",
        "By Yann LeChun et al. (1998)\n",
        "\n",
        "http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
        "\n",
        "\n",
        "### Output value :\n",
        "\n",
        "Can be:\n",
        "  * Continuous (price)\n",
        "  \n",
        "  * Binary (will exit yes/no)\n",
        "  \n",
        "  * Categorical so it might look like bellow. \n",
        "\n",
        "  \n",
        "![alt text](https://i.imgur.com/ClOT5kp.png)\n",
        "  \n",
        "  for both the input and output we have the same single observation. \n",
        "  \n",
        "  \n",
        "###The \"Synapses\"\n",
        "\n",
        "They are the wieghts in our inputs. \n",
        "\n",
        "\n",
        "  \n",
        "![alt text](https://i.imgur.com/1KnXRNh.png)\n",
        "\n",
        "## So what's going on in the neuron?\n",
        "\n",
        "* first step is it's adding all the values \n",
        "\n",
        "![alt text](https://i.imgur.com/MWqLQvU.png)\n",
        "\n",
        "* then an activation function\n",
        "\n",
        "![alt text](https://i.imgur.com/qI5rqvY.png)\n",
        "\n",
        "* finally it passes on to the next neuron down the line \n",
        "\n",
        "![alt text](https://i.imgur.com/s29kDkg.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHKaNpOiuSKy",
        "colab_type": "text"
      },
      "source": [
        "# The Activation Function (aka the third step.)\n",
        "\n",
        "## Four Primary Types: \n",
        "\n",
        "### Threshold Function\n",
        "\n",
        "![alt text](https://i.imgur.com/z129rEm.png)\n",
        "\n",
        "Good for binary true false yes no cases\n",
        "\n",
        "### Sigmoid Function\n",
        "\n",
        "![alt text](https://i.imgur.com/Tm68zmd.png)\n",
        "\n",
        "* Used for logistic regression\n",
        " * smooth better used for probability problems. \n",
        " \n",
        " ### Rectifier Function\n",
        "\n",
        "![alt text](https://i.imgur.com/EQzrc4s.png)\n",
        "\n",
        "* Most commonly used\n",
        "* for values expected to increase \n",
        "\n",
        "### Hyperbolic Tangent (tanh)\n",
        "\n",
        "![alt text](https://i.imgur.com/RjLtRkM.png)\n",
        "\n",
        "* Like sigmoid, but can go bellow 0\n",
        "\n",
        "Additional Reading:\n",
        "\n",
        "Deep Sparse Rectifier Neural Networks\n",
        "\n",
        "by Xavier Glorot et al. (2011)\n",
        "\n",
        "http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXCRJUkyok1x",
        "colab_type": "text"
      },
      "source": [
        "Quick exercise. \n",
        "\n",
        "Assuming the DV (Depndent variable is binary) Which threshold would i choose? Threshold function. possibly the sigmoid if there is need to estimate probability. \n",
        "\n",
        "![alt text](https://i.imgur.com/Q3vdXsY.png)\n",
        "\n",
        "usually in the hidden layer we will apply the rectifier function followed by whatever required function for output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hubp8C0BxRfe",
        "colab_type": "text"
      },
      "source": [
        "## How do NNs Work?\n",
        "\n",
        "Lots of images ahead in this. Just warning everyone now. We are going to estimate property values. \n",
        "\n",
        "\n",
        "In it's basic form the neural network has it's input and output layer. \n",
        "\n",
        "![alt text](https://i.imgur.com/rnmunHl.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZQdHBesydv1",
        "colab_type": "text"
      },
      "source": [
        "In this primitive form the input would be weighed by it's 'synapses' and simply calculated from there. \n",
        "\n",
        "![alt text](https://i.imgur.com/37zLuYB.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYix4t6-y_ME",
        "colab_type": "text"
      },
      "source": [
        "This of course doesn't mean we will get accurate results, but it's imporessive we get an output at all on this primitive level. So from there of course we add the hidden layers. From there we will break it down further. \n",
        "\n",
        "![alt text](https://i.imgur.com/dEKsTbf.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcZPcEiKznHg",
        "colab_type": "text"
      },
      "source": [
        "So to start all 4 variables on the left. and we start on the fist neuron on the top of the hidden layer. All of which will have weights. Some will have non zero values others will have a zero value. remember not all data has as much importance or correlation as other data has. \n",
        "\n",
        "![alt text](https://i.imgur.com/3bED9YP.png)\n",
        "\n",
        "\n",
        "### For the sake of simplicity in the next set of images zero values will not have their dotted lines there for the sake of simplifying our images and removing diversions of where our focus should be. \n",
        "\n",
        "![alt text](https://i.imgur.com/e34g6k9.png)\n",
        "\n",
        "\n",
        "Got it? Good. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhXs9N9qzqez",
        "colab_type": "text"
      },
      "source": [
        "So now we will connect to another neuron. Why are these connecting? \n",
        "\n",
        "![alt text](https://i.imgur.com/eIKzu1P.png)\n",
        "\n",
        "Looks like we have another set of coorelations found. \n",
        "\n",
        "So now lets look at the bottom one. \n",
        "\n",
        "![alt text](https://i.imgur.com/CiVVEQP.png)\n",
        "\n",
        "Looks like age itself is a significant here in it's own right. Looks like a good place for a rectifier function could be applied here. \n",
        "\n",
        "![alt text](https://i.imgur.com/4zQ5jNV.png)\n",
        "\n",
        "So the inputs in short are finding connections based on correlations they apply to each neuron on the hidden layer. These can be applied and suited for our given task. From there we add the weigts and dropouts to iron out things, and it goes to the output from there. \n",
        "\n",
        "![alt text](https://i.imgur.com/WZTkcIJ.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3V0HcOfzrGf",
        "colab_type": "text"
      },
      "source": [
        "# How Do NNs Learn?\n",
        "\n",
        "Here is our perceptron or single layer neural network. \n",
        "\n",
        "![alt text](https://i.imgur.com/QFT6Prp.png)\n",
        "\n",
        "on the left the input values run though the neural network, the activation function is applied and we get our output and it's plotted on the chart.\n",
        "\n",
        "From there we compare to the actual value we want. We from there calculate the cost function which is basically a glorified difference between the toutput value and acutal value whether positive or negative. to make sure the machine fits to the training data. The closer it is to the actual value the better off we are to yielding correct results. \n",
        "\n",
        "![alt text](https://i.imgur.com/D4POcEg.png)\n",
        "\n",
        "Once compared we feed the info to our neural network and make corrections updating our weights in the given model. \n",
        "\n",
        "![alt text](https://i.imgur.com/1nGBC8a.png)\n",
        "\n",
        "now in this example we are dealing with just a single row. There will be other rows to deal with. For the sake of simplicty we are only dealing with one row. Now lets see what running a bunch of epochs looks like.\n",
        "\n",
        "![alt text](https://i.imgur.com/HlHJSfR.png)\n",
        "\n",
        "from there we check the cost function and update the weights \n",
        "\n",
        "![alt text](https://i.imgur.com/NqLSy9I.png)\n",
        "\n",
        "All rows share the weights. \n",
        "\n",
        "**Additional reading :**\n",
        "\n",
        "A list of cost functions used in neural networks, alongside applications\n",
        "\n",
        "link:\n",
        "\n",
        "https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyN_PeJvzsKk",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Descent\n",
        "\n",
        "\"So how are those weights adjusted in our neural network?\" Good question.\n",
        "\n",
        "So if we wanted to brute force things we'd keep plotting until we hit the lowest point possible on our input values. But this tanks our efficiency and slows us down significantly the more complexity we add to our neural network. This is the curse of dimensionality.\n",
        "\n",
        "So instead we will use gradient descent. We wrote out a lot of this in our ealier machine learning notes so we will refer to those notes. \n",
        "\n",
        "# Sotchastic Gradient Descent\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWStsJqrzsgN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwGLo6DDzs7n",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L10WryIztSy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPFzzEjGztsX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}