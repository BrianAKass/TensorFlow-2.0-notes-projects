{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06-01 Reinforcement Learning.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGOOKwrDJ2Od",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement learning\n",
        "\n",
        "Lesson begins with slidessow of robot and maze explaining the concepts of enviornment, action state and reward the agent has to deal with. \n",
        "\n",
        "Goes on to explain how rewarding the good actions forwards the progress of our model. \n",
        "\n",
        "Compares contrasts with pre programmed models.\n",
        "\n",
        "Additional reading:\n",
        "\n",
        "Simple reinforcement learning with tensorflow (10 parts) by Arthur Juliani (2016)\n",
        "\n",
        "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
        "\n",
        "Reinforcement learning I: Introduction by Richard Sutton et al. (1998)\n",
        "\n",
        "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYkj4MiINeUF",
        "colab_type": "text"
      },
      "source": [
        "## Bellman Equation\n",
        "\n",
        "Concepts:\n",
        "\n",
        "    * s - State - State which our agent is in. \n",
        "    \n",
        "    * a - Action - Actions that an agent can do usually based on current state\n",
        "    \n",
        "    * R - Reward - Reward agent gets when entering a certain state. \n",
        "    \n",
        "    * Î³ - Discount\n",
        "    \n",
        "    Richard Ernest Bellman made this math possible. Respect. \n",
        "    \n",
        "![alt text](https://i.imgur.com/0XmkhZx.png)\n",
        "    \n",
        "    Lots of robot trial and error comes about from this. Looks at the last step before that got it there and uses that as part of it's education. marks in this example the square before green as v=1, along with te ones before that. of course this method doesnt work if the agent appears in a new place and has no idea which mark is best mark how doe we fix this? Bellman gives us the answer!\n",
        "    \n",
        "![alt text](https://i.imgur.com/jjiJ7Uh.png)\n",
        "\n",
        "Value of the crrent state is equal to the max action Reward times state, action plus the discounted value of the prime state (The state you will end up in after this given state.)\n",
        "\n",
        "Bellman basically created marco polo in math for us. Awesome. \n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/Ea5yQ7F.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qklr1sWVYK5T",
        "colab_type": "text"
      },
      "source": [
        "# Markov Decision Process (MDP)\n",
        "\n",
        "\n",
        "### Deterministic Search vs Non Deterministic\n",
        "\n",
        "![alt text](https://i.imgur.com/2WSCJmD.png)\n",
        "\n",
        "On left 100% change choosing up will have agent go up. RIght means theres small chances that might not be the case.\n",
        "\n",
        "Markov Process  vs Markov Decision Process.\n",
        "\n",
        "\n",
        "A stochastic process (randomly determined; having a random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely.) has the Markov property if the condiotional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that predicted it. A process with his porperty is called a **Markov process**\n",
        "\n",
        "In short it's when your choice and enviornment only depend on where you are present, and not before or after how you got there. it's only making the decision based on the present not past actions. \n",
        "\n",
        "**A Markov Deceision Process (MDPs)** provide a mathmatical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.\n",
        "\n",
        "If this is a factor in our model Bellman's equation gets a little dicier.  we basically are dealing with multiple S primes and determiningour expected values. \n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/S2EerFB.png)\n",
        "\n",
        "\n",
        "Then we take a load of this equation: \n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/r6Lpg34.png)\n",
        "\n",
        "Its the same as above just more mathmatically written correctly. the sum of our probability states in relation to our dicount and the value of state prime. \n",
        "\n",
        "\n",
        "Moar reading:\n",
        "\n",
        "Survey of Applications of markov Decision Process.\n",
        "\n",
        "https://www.cs.uml.edu/ecg/uploads/AIfall16/MDPApplications3.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KrgXTVVi6_m",
        "colab_type": "text"
      },
      "source": [
        "# Q-Learning Intuition\n",
        "\n",
        "![alt text](https://i.imgur.com/zZODQEi.png)\n",
        "\n",
        "Q represents the quality of the given action. If it moves us closer further away and other things. \n",
        "\n",
        "![alt text](https://i.imgur.com/729OiMM.png)\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/0kGsKaV.png)\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/SMXk5GW.png)\n",
        "\n",
        "\n",
        "MOAR READING:\n",
        "\n",
        "Markov Decision Process:\n",
        "\n",
        "Concepts and Algorithms\n",
        "\n",
        "By Martin van Otterlo (2009)\n",
        "\n",
        "https://pdfs.semanticscholar.org/968b/ab782e52faf0f7957ca0f38b9e9078454afe.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk-54RQOr9ay",
        "colab_type": "text"
      },
      "source": [
        "# Temporal Difference\n",
        "\n",
        "for the sake of simplicity we will simplify the Q learning equation to the bottom equation. \n",
        "\n",
        "![alt text](https://i.imgur.com/Ra6Rpsl.png)\n",
        "\n",
        "![alt text](https://i.imgur.com/ezz1v1W.png)\n",
        "\n",
        "![alt text](https://i.imgur.com/n7BNsQZ.png)\n",
        "\n",
        "Think 4th dimensionally with the temporal displacement. We are taking the new Q learning updated value and subtracting by the old to get the temporal difference.  makes sense. \n",
        "\n",
        "https://www.youtube.com/watch?v=9-B7INVbdII\n",
        "\n",
        "![alt text](https://i.imgur.com/5xrcc4K.png)\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/mYv7VXz.png)\n",
        "\n",
        "We take the old value of Q(s,a) added by the alpha temporal difference of agnt and state. taking our temporal difference and adding it to our previous Q(s,a). Lets throw in temporal variable to clear up the equation. \n",
        "\n",
        "![alt text](https://i.imgur.com/NLGMIJM.png)\n",
        "\n",
        "\n",
        "Here the final equation with all the bells and whistles added to it. \n",
        "\n",
        "![alt text](https://i.imgur.com/CaCC2aK.png)\n",
        "\n",
        "\n",
        "Moar reading:\n",
        "\n",
        "Learning to Predict by the Methods of Temporal Differences \n",
        "\n",
        "Richard Sutton (1988)\n",
        "\n",
        "link:\n",
        "\n",
        "https://link.springer.com/content/pdf/10.1007%2FBF00115009.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PABpufwA-PeO",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q-Learning Intuition  \n",
        "\n",
        "Now for the maze we add the x1 and x2 values values so it can be translated to a neural network and said network outputs 4 Q values for our equation. Remember we gotta simplify the nural network to reduce error.\n",
        "\n",
        "![alt text](https://i.imgur.com/UZ2xFLe.png)\n",
        "\n",
        "how do we adapt the concept of temporal difference to a neural network so that we can leverage the full power of neural networks?\n",
        "\n",
        "![alt text](https://i.imgur.com/J72jqtf.png)\n",
        "\n",
        "from the equation Loss equals the sum of Q on the output neural network  minus Q target, squared. \n",
        "\n",
        "\n",
        "[link text](https://i.imgur.com/J72jqtf.png)\n",
        "\n",
        "Rememer we want to simplifythis eqation which ideally whould go down to zero meaning we know what that is and where we are. \n",
        "\n",
        "Finally we run it through the network and the agent improves.\n",
        "\n",
        "After learning, acting comes next. We put the agent through te softmax layer, thus selecting the best option possible. \n",
        "\n",
        "![alt text](https://i.imgur.com/uV6ubi2.png) \n",
        "\n",
        "This whole process is a single epoch in a basic Deep Q-learning setup sometimes every state as well.  \n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/noUYEW5.png)\n",
        "\n",
        "\n",
        "More reading:\n",
        "simple Reinforcement Learning with TensorFlow (part 4)\n",
        "\n",
        "Arthur Juliani (2016)\n",
        "\n",
        "\n",
        "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fgrl-fFn_cv",
        "colab_type": "text"
      },
      "source": [
        "# Experience Replay\n",
        "\n",
        "![alt text](https://i.imgur.com/PwlXVx2.png)\n",
        "\n",
        "in this example we are having a car drive on thie \"road\"\n",
        "\n",
        "as the agent progresses we run the neural network, thing is if not much changes (like being in a straightaway) it's not going to really learn anything new. Having for lack of a better word belead into a false sense of security for it's constant rewards of each update and no crash. \n",
        "\n",
        "This is where **Experiece Replay** comes in. those 50 or so reated lessons dont get applied to our network right away, but saved into the memory of the agent, and then uses that batch to attempt to learn further. taking a uniformly distributed sample, then goes through it and learns. Breaking the pattern of the bias. \n",
        "\n",
        "In these models still the more rare occurances are the harder ones to learn from. It simply needs more experience to be on par with other scenarios. \n",
        "\n",
        "\n",
        "more reading\n",
        "\n",
        "Prioritized Experience replay \n",
        "\n",
        "Tom Schaul et al.,\n",
        "\n",
        "Google DeepMind (2016)\n",
        "\n",
        "\n",
        "link:\n",
        "\n",
        "https://arxiv.org/pdf/1511.05952.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU3UkDkRwT3a",
        "colab_type": "text"
      },
      "source": [
        "# Action Selection Policies\n",
        "\n",
        "Lets focus more on the acting part of the model. \n",
        "\n",
        "![alt text](https://i.imgur.com/jPX13P5.png)\n",
        "\n",
        "So how exactly does the softmax layer determine which q value to choose? The highest Q value is normally the default but when it comes to softmax layers Action selection policies come into play. \n",
        "\n",
        "The usual suspects in our action selection are $\\epsilon$-greedy, $\\epsilon$-soft (1-$\\epsilon$), and Softmax. So why do we need different kinds? Exploration Vs Exploitation amigo. \n",
        "\n",
        "We might get stuck on the same q values over time when another q value might be a better fit for us. \n",
        "\n",
        "$\\epsilon$-greedy Selects the highest q-value except for $\\epsilon$ percent of the time which will then pick a random q value instead. \n",
        "\n",
        "$\\epsilon$-soft  (1-$\\epsilon$) opposite of greedy only picking higest q-value $\\epsilon$% of the time\n",
        "\n",
        "softmax is a more advanced version of $\\epsilon$-greedy. Highest q value most of the time occasionally in a blue moon going for a random one. \n",
        "\n",
        "![alt text](https://i.imgur.com/PXrgr41.png)\n",
        "\n",
        "Upper right is the softmax equation. Turning all values to either 0 or 1 \n",
        "\n",
        "![alt text](https://i.imgur.com/DI8drvv.png)\n",
        "\n",
        "as we can see he softmax is reading the q values which turns into its percentages and uses them to gause how often it should learn from each q value. 90%, 5% you get the idea. \n",
        "\n",
        "\n",
        "More reading \n",
        "\n",
        "Adaptive $\\epsilon$-greedy Exploration in Reinforcement Learning based on Value Differences\n",
        "\n",
        "Michel Tokic(2010) \n",
        "\n",
        "Link: \n",
        "\n",
        "http://tokic.com/www/tokicm/publikationen/papers/AdaptiveEpsilonGreedyExploration.pdf"
      ]
    }
  ]
}