{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06-01 Reinforcement Learning.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdDzKVjUGmeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGOOKwrDJ2Od",
        "colab_type": "text"
      },
      "source": [
        "Reinforcement learning\n",
        "\n",
        "Lesson begins with slidessow of robot and maze explaining the concepts of enviornment, action state and reward the agent has to deal with. \n",
        "\n",
        "Goes on to explain how rewarding the good actions forwards the progress of our model. \n",
        "\n",
        "Compares contrasts with pre programmed models.\n",
        "\n",
        "Additional reading:\n",
        "\n",
        "Simple reinforcement learning with tensorflow (10 parts) by Arthur Juliani (2016)\n",
        "\n",
        "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
        "\n",
        "Reinforcement learning I: Introduction by Richard Sutton et al. (1998)\n",
        "\n",
        "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYkj4MiINeUF",
        "colab_type": "text"
      },
      "source": [
        "## Bellman Equation\n",
        "\n",
        "Concepts:\n",
        "\n",
        "    * s - State - State which our agent is in. \n",
        "    \n",
        "    * a - Action - Actions that an agent can do usually based on current state\n",
        "    \n",
        "    * R - Reward - Reward agent gets when entering a certain state. \n",
        "    \n",
        "    * Î³ - Discount\n",
        "    \n",
        "    Richard Ernest Bellman made this math possible. Respect. \n",
        "    \n",
        "![alt text](https://i.imgur.com/0XmkhZx.png)\n",
        "    \n",
        "    Lots of robot trial and error comes about from this. Looks at the last step before that got it there and uses that as part of it's education. marks in this example the square before green as v=1, along with te ones before that. of course this method doesnt work if the agent appears in a new place and has no idea which mark is best mark how doe we fix this? Bellman gives us the answer!\n",
        "    \n",
        "![alt text](https://i.imgur.com/jjiJ7Uh.png)\n",
        "\n",
        "Value of the crrent state is equal to the max action Reward times state, action plus the discounted value of the prime state (The state you will end up in after this given state.)\n",
        "\n",
        "Bellman basically created marco polo in math for us. Awesome. \n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/Ea5yQ7F.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qklr1sWVYK5T",
        "colab_type": "text"
      },
      "source": [
        "# Markov Decision Process (MDP)\n",
        "\n",
        "\n",
        "### Deterministic Search vs Non Deterministic\n",
        "\n",
        "![alt text](https://i.imgur.com/2WSCJmD.png)\n",
        "\n",
        "On left 100% change choosing up will have agent go up. RIght means theres small chances that might not be the case.\n",
        "\n",
        "Markov Process  vs Markov Decision Process.\n",
        "\n",
        "\n",
        "A stochastic process (randomly determined; having a random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely.) has the Markov property if the condiotional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that predicted it. A process with his porperty is called a **Markov process**\n",
        "\n",
        "In short it's when your choice and enviornment only depend on where you are present, and not before or after how you got there. it's only making the decision based on the present not past actions. \n",
        "\n",
        "**A Markov Deceision Process (MDPs)** provide a mathmatical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.\n",
        "\n",
        "If this is a factor in our model Bellman's equation gets a little dicier.  we basically are dealing with multiple S primes and determiningour expected values. \n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/S2EerFB.png)\n",
        "\n",
        "\n",
        "Then we take a load of this equation: \n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/r6Lpg34.png)\n",
        "\n",
        "Its the same as above just more mathmatically written correctly. the sum of our probability states in relation to our dicount and the value of state prime. \n",
        "\n",
        "\n",
        "Moar reading:\n",
        "\n",
        "Survey of Applications of markov Decision Process.\n",
        "\n",
        "https://www.cs.uml.edu/ecg/uploads/AIfall16/MDPApplications3.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KrgXTVVi6_m",
        "colab_type": "text"
      },
      "source": [
        "# Q-Learning Intuition\n",
        "\n",
        "![alt text](https://i.imgur.com/zZODQEi.png)\n",
        "\n",
        "Q represents the quality of the given action. If it moves us closer further away and other things. "
      ]
    }
  ]
}