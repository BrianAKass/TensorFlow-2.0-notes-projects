{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07-03 Reinforcement Learning for Stock Market Trading.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BW67to5SwJa",
        "colab_type": "text"
      },
      "source": [
        "## Stage 1: Installing dependencies and enviornment setup for stock trading. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGx3o9GVSgpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "eae83bde-7ca5-4a34-e31e-206068696cd1"
      },
      "source": [
        "## Google just rolled out a new successor to 2.0.0 beta colab only.\n",
        "#might have to fix this later to get things working. \n",
        "# version still outpusts to 2.0.0-beta 1 so ill stick with the standard pip for now.\n",
        "# that way i can send it in theory to jupyter later if need be. \n",
        "\n",
        "try:\n",
        "  #%tensorflow_version 2.x  # Colab only. \n",
        "  !pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "!pip install pandas-datareader"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.x  # Colab only.`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.6/dist-packages (0.7.4)\n",
            "Requirement already satisfied: wrapt in /tensorflow-2.0.0b1/python3.6 (from pandas-datareader) (1.11.2)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (0.24.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (4.2.6)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader) (2.21.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas-datareader) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /tensorflow-2.0.0b1/python3.6 (from pandas>=0.19.2->pandas-datareader) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->pandas-datareader) (2.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->pandas-datareader) (2019.6.16)\n",
            "Requirement already satisfied: six>=1.5 in /tensorflow-2.0.0b1/python3.6 (from python-dateutil>=2.5.0->pandas>=0.19.2->pandas-datareader) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIOnd7hpVAe0",
        "colab_type": "text"
      },
      "source": [
        "## Step 2 Import Dependencies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtOZbTs-TIMm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a84707b4-3702-447b-b3a5-2ed81b56ca80"
      },
      "source": [
        "import math #need for sigmoid functions\n",
        "import random # generating random numbers\n",
        "import numpy as np # helps with arrays and matrices\n",
        "import pandas as pd #reading dataframes and reading the csv we will use\n",
        "import tensorflow as tf # the big guns\n",
        "import matplotlib.pyplot as plt #data visualization\n",
        "import pandas_datareader as data_reader #download and use stock info\n",
        "\n",
        "from tqdm import tqdm_notebook, tqdm #visualizes our progress\n",
        "from collections import deque # implements experience replay.\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-beta1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibSFprJscpvR",
        "colab_type": "text"
      },
      "source": [
        "## Step 3 building an AI network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAA8VBemUELt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AI_Trader():\n",
        "  \n",
        "  def __init__(self, state_size, action_space = 3, model_name = \"AITrader\"): #Three actions: Stay, Buy, and Sell\n",
        "    \n",
        "    self.state_size = state_size\n",
        "    self.action_space = action_space\n",
        "    \n",
        "    #more model realted params\n",
        "    #experience replay memory\n",
        "    self.memory = deque(maxlen=2000) # how many elements we can store inside experience replay\n",
        "    self.inventory = [] # Blank list to hold all our stocks\n",
        "    self.model_name = model_name\n",
        "    \n",
        "    #see reinforcement learning notes to jog memory of these formulas \n",
        "    self.gamma = 0.95 # maximizes current reward over longtime reward\n",
        "    self.epsilon = 1.0 # determines whether to choose random action, or model. We choose random before trained in this case\n",
        "    self.epsilon_final = 0.01 #when equal to or less we will stop decreasing it.\n",
        "    self.epsilon_decay = 0.995 #must be less than 1\n",
        "    \n",
        "    self.model = self.model_builder()\n",
        "  \n",
        "  def model_builder(self):\n",
        "    \n",
        "    model = tf.keras.model.Sequential()\n",
        "    \n",
        "    # hidden layers\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(units = 32, activation='relu', input_dim = self.state_size))\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(units = 64, activation='relu'))\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(units = 128, activation='relu'))\n",
        "    \n",
        "    # output layer\n",
        "    \n",
        "    model.add(tf.keras.layers.dense(units = self.action_state, activation = 'linear')) #change acivation to linear for mean squared error\n",
        "    \n",
        "    #compile\n",
        "    # since this is regression and not classification, cant use accuracy as metric, we leave that empty\n",
        "    model.compile(loss='mse', optimizer = tf.keras.optimizers.Adam(lr=0.001)) #lr = learning rate\n",
        "    \n",
        "    return model\n",
        "  \n",
        "  #build trade function that takes state and performs an action based on it. \n",
        "  def trade(self,state):\n",
        "    \n",
        "    if random random() <= self.epsilon:\n",
        "      #returns random action\n",
        "      return random.randrange(self.action_space)\n",
        "    #if random number is bigger than epsilon we use our model to choose an action to perform\n",
        "    actions = self.model.predict(state)\n",
        "    return np.argmax(actions[0])\n",
        "  \n",
        "  #train the model in batches\n",
        "  def batch_trade(self, batch_size):\n",
        "    #select data from experiance replay memory\n",
        "    batch = []\n",
        "    #iteration time append recent stock memory to batch\n",
        "    for i in range (len(self.memory) - batch_size + 1, len(self.memory)):\n",
        "      batch.append(self.memory[i])\n",
        "      \n",
        "    #iterate the batch\n",
        "    \n",
        "    #remember the variable s to the temporal differential equation in the previous notes. \n",
        "    for state, action, reward, next_state, done in batch:\n",
        "      reward = reward\n",
        "      if not done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VqE1I5JvKa0",
        "colab_type": "text"
      },
      "source": [
        "## Step 4 Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8TnQvcGvTGr",
        "colab_type": "text"
      },
      "source": [
        "### Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB72Ywi2uD-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}