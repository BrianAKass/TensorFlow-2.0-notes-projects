{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07-03 Reinforcement Learning for Stock Market Trading.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BW67to5SwJa",
        "colab_type": "text"
      },
      "source": [
        "## Stage 1: Installing dependencies and enviornment setup for stock trading. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGx3o9GVSgpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Google just rolled out a new successor to 2.0.0 beta colab only.\n",
        "#might have to fix this later to get things working. \n",
        "# version still outpusts to 2.0.0-beta 1 so ill stick with the standard pip for now.\n",
        "# that way i can send it in theory to jupyter later if need be. \n",
        "\n",
        "try:\n",
        "  #%tensorflow_version 2.x  # Colab only. \n",
        "  !pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "!pip install pandas-datareader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIOnd7hpVAe0",
        "colab_type": "text"
      },
      "source": [
        "## Step 2 Import Dependencies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtOZbTs-TIMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math #need for sigmoid functions\n",
        "import random # generating random numbers\n",
        "import numpy as np # helps with arrays and matrices\n",
        "import pandas as pd #reading dataframes and reading the csv we will use\n",
        "import tensorflow as tf # the big guns\n",
        "import matplotlib.pyplot as plt #data visualization\n",
        "import pandas_datareader as data_reader #download and use stock info\n",
        "\n",
        "from tqdm import tqdm_notebook, tqdm #visualizes our progress\n",
        "from collections import deque # implements experience replay.\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibSFprJscpvR",
        "colab_type": "text"
      },
      "source": [
        "## Step 3 building an AI network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAA8VBemUELt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AI_Trader():\n",
        "  \n",
        "  def __init__(self, state_size, action_space = 3, model_name = \"AITrader\"): #Three actions: Stay, Buy, and Sell\n",
        "    \n",
        "    self.state_size = state_size\n",
        "    self.action_space = action_space\n",
        "    \n",
        "    #more model realted params\n",
        "    #experience replay memory\n",
        "    self.memory = deque(maxlen=2000) # how many elements we can store inside experience replay\n",
        "    self.inventory = [] # Blank list to hold all our stocks\n",
        "    self.model_name = model_name\n",
        "    \n",
        "    #see reinforcement learning notes to jog memory of these formulas \n",
        "    self.gamma = 0.95 # maximizes current reward over longtime reward\n",
        "    self.epsilon = 1.0 # determines whether to choose random action, or model. We choose random before trained in this case\n",
        "    self.epsilon_final = 0.01 #when equal to or less we will stop decreasing it.\n",
        "    self.epsilon_decay = 0.995 #must be less than 1\n",
        "    \n",
        "    self.model = self.model_builder()\n",
        "  \n",
        "  def model_builder(self):\n",
        "    \n",
        "    model = tf.keras.models.Sequential() #models not model get the syntax right\n",
        "    \n",
        "    # hidden layers\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(units = 32, activation='relu', input_dim = self.state_size))\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(units = 64, activation='relu'))\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(units = 128, activation='relu'))\n",
        "    \n",
        "    # output layer\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(units = self.action_space, activation = 'linear')) #change acivation to linear for mean squared error\n",
        "    \n",
        "    #compile\n",
        "    # since this is regression and not classification, cant use accuracy as metric, we leave that empty\n",
        "    model.compile(loss='mse', optimizer = tf.keras.optimizers.Adam(lr=0.001)) #lr = learning rate\n",
        "    \n",
        "    return model\n",
        "  \n",
        "  #build trade function that takes state and performs an action based on it. \n",
        "  def trade(self,state):\n",
        "    \n",
        "    if random.random() <= self.epsilon:\n",
        "      #returns random action\n",
        "      return random.randrange(self.action_space)\n",
        "    #if random number is bigger than epsilon we use our model to choose an action to perform\n",
        "    actions = self.model.predict(state)\n",
        "    return np.argmax(actions[0])\n",
        "  \n",
        "  #train the model in batches\n",
        "  def batch_train(self, batch_size):\n",
        "    #select data from experiance replay memory\n",
        "    batch = []\n",
        "    #iteration time append recent stock memory to batch\n",
        "    for i in range(len(self.memory) - batch_size + 1, len(self.memory)):\n",
        "      batch.append(self.memory[i])\n",
        "      \n",
        "    #iterate the batch\n",
        "    \n",
        "    #remember the variable s to the temporal differential equation in the previous notes. \n",
        "    for state, action, reward, next_state, done in batch:\n",
        "      #if agent is in a terminal state we will use current reward as reward\n",
        "      reward = reward\n",
        "      if not done: #not in terminal state and there are a few more actions to be played\n",
        "        # if not in terminal state and there are few more actions to be played calculate discounted terminal reward as reward        \n",
        "        reward = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "      \n",
        "      target = self.model.predict(state) #just an action. must modify with current reward\n",
        "      target[0][action] = reward\n",
        "      \n",
        "      #feed the model\n",
        "      self.model.fit(state, target, epochs=1, verbose=0)\n",
        "      \n",
        "    #decrease epsilon param to stop performing random actions eventaully \n",
        "    if self.epsilon > self.epsilon_final:\n",
        "      self.epsilon *= self.epsilon_decay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VqE1I5JvKa0",
        "colab_type": "text"
      },
      "source": [
        "## Step 4 Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8TnQvcGvTGr",
        "colab_type": "text"
      },
      "source": [
        "### Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB72Ywi2uD-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scales to either 0 or 1 for binary classification\n",
        "def sigmoid(x):\n",
        "  # scale prices to compare and gather real differneces between each day\n",
        "  return 1 / (1+math.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ZDmqzOB1Np",
        "colab_type": "text"
      },
      "source": [
        "### Price format function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMSlHnvaB4aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stocks_price_format(n):\n",
        "  if n < 0 :\n",
        "  #returns positive or negative\n",
        "    return \"- $ {0:2f}\".format(abs(n))\n",
        "  else:\n",
        "    return \"$ {0:2f}\".format(abs(n))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEVQQBc5B4vh",
        "colab_type": "text"
      },
      "source": [
        "### Dataset loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E0XdiTKDoZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset = data_reader.DataReader(\"AAPL\", data_source='yahoo')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m3JWZEbB7_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataset_loader(stock_name):\n",
        "  \n",
        "  dataset = data_reader.DataReader(stock_name, data_source='yahoo')\n",
        "  \n",
        "  start_date = str(dataset.index[0]).split()[0]\n",
        "  end_date = str(dataset.index[-1]).split()[0]\n",
        "  \n",
        "  close = dataset['Close']\n",
        "  \n",
        "  return close"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVLS7g2v-Rx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = data_reader.DataReader(\"AAPL\", data_source='yahoo')\n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1kctAdzcIY5",
        "colab_type": "text"
      },
      "source": [
        "### State creator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW_lIY2WZGZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def state_creator(data, timestep, window_size):\n",
        "  \n",
        "  starting_id = timestep - window_size + 1\n",
        "  \n",
        "  if starting_id >= 0:\n",
        "    windowed_data = data[starting_id: timestep+1]\n",
        "  else:\n",
        "    windowed_data = - starting_id * [data[0]] + list(data[0:timestep+1])\n",
        "    \n",
        "  state = []\n",
        "  for i in range(window_size - 1):\n",
        "    state.append(sigmoid(windowed_data[i+1] - windowed_data[i]))\n",
        "    \n",
        "  return np.array([state])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6lYyDDAy5RH",
        "colab_type": "text"
      },
      "source": [
        "### Loading a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuLkhuZHZ04E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stock_name = \"AAPL\"\n",
        "data = dataset_loader(stock_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrgV_1lY-mdI",
        "colab_type": "text"
      },
      "source": [
        "## Stage 5: Training the AI Trader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPRq8TbJrAiX",
        "colab_type": "text"
      },
      "source": [
        "### Setting hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kR86BfI-_LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 10 # previous number of of days to predict the current \n",
        "episodes = 1000 #same as epochs\n",
        "\n",
        "batch_size = 32\n",
        "data_samples = len(data) - 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XREqz2VR-6pG",
        "colab_type": "text"
      },
      "source": [
        "### Defining the Trader model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxVFu8AI_A1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#aka assigning the big class we wrote Just need to specify state_size which is our window_size\n",
        "trader = AI_Trader(window_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZiTkW_sB9VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trader.model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg2KbySBD8WP",
        "colab_type": "text"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf9Guq79CAWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episode in range(episodes+1):  \n",
        "  \n",
        "  print(f\"Episode: {episode}/{episodes}\")\n",
        "  \n",
        "  #current timestep is 0\n",
        "  state = state_creator(data, 0, window_size + 1)\n",
        "  \n",
        "  total_profit = 0\n",
        "  #empty list to append all stocks bought\n",
        "  trader.inventory = []\n",
        "  \n",
        "  # define our timestamp \n",
        "  #tqdm is used to visualize the progress bar\n",
        "  for t in tqdm(range(data_samples)):\n",
        "    #access action taken by the model\n",
        "    action = trader.trade(state)\n",
        "    \n",
        "    next_state = state_creator(data, t+1, window_size + 1)\n",
        "    \n",
        "    reward = 0\n",
        "    \n",
        "    if action == 1: #buying\n",
        "      trader.inventory.append(data[t]) #current stock added to inventory\n",
        "      print(\"AI Trader bought: \", stocks_price_format(data[t]))\n",
        "      \n",
        "    elif action == 2 and len(trader.inventory) > 0: #selling \n",
        "      buy_price = trader.inventory.pop(0)\n",
        "      print(\"AI Trader sold: \", stocks_price_format(data[t]), \" Profit: \" + stocks_price_format(data[t] - buy_price) )\n",
        "      \n",
        "      \n",
        "      reward = max(data[t] - buy_price, 0)\n",
        "      total_profit += data[t] - buy_price\n",
        "    if t == data_samples - 1:\n",
        "      done = True\n",
        "    else:\n",
        "      done = False\n",
        "    \n",
        "    #append \n",
        "    trader.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    #change the state to the next state\n",
        "    \n",
        "    state = next_state\n",
        "    \n",
        "    if done:\n",
        "      print(\"######################\")\n",
        "      print(f\"TOTAL PROFIT: {total_profit}\")\n",
        "      print(\"######################\")\n",
        "      \n",
        "    if len(trader.memory) > batch_size:\n",
        "      trader.batch_train(batch_size)\n",
        "  if episode % 10 == 0:\n",
        "    trader.model.save(f\"ai_trader_{episode}.h5 \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbnlZFixLidM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}