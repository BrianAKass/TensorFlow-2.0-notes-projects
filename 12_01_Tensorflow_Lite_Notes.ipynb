{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12-01 Tensorflow Lite Notes.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-MUdDAZRlPm",
        "colab_type": "text"
      },
      "source": [
        "* Extension of the tensorflow library used to convert to mobile devices\n",
        "\n",
        "* how do we optimize for mobile?\n",
        "\n",
        "    * have to deal with model size\n",
        "    \n",
        "    * neural network\n",
        " \n",
        "Deep learning on mobile phones workflow:\n",
        "  \n",
        "  * Train model first on computer.\n",
        "  \n",
        "  * Optimize model with TF Lite\n",
        "  \n",
        "  * interference time on a mobile device. \n",
        "  \n",
        "  \n",
        "  ![alt text](https://i.imgur.com/YdRPVzd.png)\n",
        "  \n",
        "  ## Model Optimization\n",
        "  \n",
        "  * Optimize model topology (faster network, smaller, etc.)\n",
        "  \n",
        "  * Reduce network parameters precision with quantization. \n",
        "  \n",
        "  * Reduce parameter count \n",
        "  \n",
        "  * 8 bit quantization and why it works https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd?gi=18450f8a5cc9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRydTZh5RmQe",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}