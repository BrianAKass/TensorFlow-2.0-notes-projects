{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13-01 Distributed Training notes..ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd5EMNpdLzAT",
        "colab_type": "text"
      },
      "source": [
        "# Distributed Training.\n",
        "\n",
        "* Ways to train a single model on multiple GPUs/CPUs/computers/servers\n",
        "\n",
        "### Why?\n",
        "\n",
        "* Because we will always have an insatiable appetite for harware beyond our reach and an impatience to fine tune software more efficiently. \n",
        "\n",
        "* even if we optimize our software, might take days/weeks to process otherwise.  \n",
        "\n",
        "* Distrubuted training lightens the load. \n",
        "\n",
        "* differnet kinds:\n",
        "  * Synchronous\n",
        "    * train different parts of the dataset at the same time\n",
        "    * end of each epoch, gradients are agregated, used to update one model.\n",
        "  \n",
        "  * Asynchronous\n",
        "    * all workers are training at same time independantly, and updating weight asynchronosly. \n",
        "  \n",
        " ## Common distributed strategies.\n",
        " \n",
        " ### Mirrored Strategy \n",
        " \n",
        " * define a model on single computer\n",
        " \n",
        " * multiple cpus/gpus on single computer.\n",
        " \n",
        " * Copy of the same model for each device Think raid 0 \n",
        " \n",
        "![alt text](https://i.imgur.com/pzgBcUf.png)\n",
        " \n",
        " * at the end of each epoch it will upload the main copy. \n",
        "  \n",
        " * agregation on CPU or on specific dedicated GPU\n",
        " \n",
        " ### Multi-Workers Mirrored Strategy \n",
        " \n",
        " * Primary device replicates for each computer/server in the network\n",
        " \n",
        " * Each machine is called a worker. \n",
        "   * workers could have multiple devices and utilize mirrored strategey on those devices for efficiency \n",
        " \n",
        " * At the end of each epoch consider all versions of the model to update the primry one. \n",
        " \n",
        " \n",
        " ![alt text](https://i.imgur.com/NzcmbD9.png)\n",
        " \n",
        " ## A few More Strategies\n",
        " \n",
        " * TPU Strategy\n",
        "  \n",
        "  * mainly used for google cloud platform. \n",
        "  \n",
        " * ParameterServerStrategy (yes that's how they spell it)"
      ]
    }
  ]
}