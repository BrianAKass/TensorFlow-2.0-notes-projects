{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16-01 recurrent neural networks notes.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wejN71MWxWkw",
        "colab_type": "text"
      },
      "source": [
        "# What we will learn in this:\n",
        "  \n",
        "  * The idea behind Recurrent Neural Networks\n",
        "  \n",
        "  * The Vanishing Gradient Problem\n",
        "  \n",
        "  * Long Short-Term Memory (LSTM) \n",
        "  \n",
        "  * Practical Intuition\n",
        "  \n",
        "  * Extra :  LSTM Variations\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tWcPHBp0ajC",
        "colab_type": "text"
      },
      "source": [
        "# What are Recurrent Neural Networks?\n",
        "\n",
        "![alt text](https://i.imgur.com/3hEbnzI.png)\n",
        "\n",
        "Let's look at the Human Brain:\n",
        "\n",
        "![alt text](https://i.imgur.com/8zdtJ2w.png)\n",
        "\n",
        "The brain despite all it's flaws is still the best frame of reference we use to create our Neural Networks... for now.  So Best to show where we are getting our models from. \n",
        "\n",
        "**Cerebrum** -top/ large area of brain\n",
        "**Cerebellum** - bottom smaller area of the brain\n",
        "**Brainstem** - Glorified puppet stings and feelers which remind me i have chronic back pain on a second by second basis. \n",
        "**Frontal Lobe** - Short term memory. Which is where we have our inspiration for the RNNs \n",
        "**Parietal Lobe**\n",
        "**Occipital Lobe** - Images and objects. so our CNNs are mimicing this area.\n",
        "**Temporal Lobe** - Long term memory. This is how our weights work. They learn from the past and the weights continue being adjusted with each front to back propegation. \n",
        "\n",
        "\n",
        "Okay gonig back to our convolutional neural network.\n",
        "\n",
        "![alt text](https://i.imgur.com/xu87ADA.png)\n",
        "\n",
        "How do we turn this into a Recurrent Neural Network?\n",
        "\n",
        "Squash it.\n",
        "\n",
        "![alt text](https://i.imgur.com/juUGDkb.png)\n",
        "\n",
        "Everything in that model before is still there but try to imagine it as three dimensional. and inseat of looking it from the perspecitve of the previous picture, we are looking at the model from a different angle. \n",
        "\n",
        "![alt text](https://i.imgur.com/htYvHAe.png)\n",
        "\n",
        "everyone thinking third dimensionally now? Cool. Everything's still there but it's all flattened. now line up all our lines and rotate 90 degrees counter clockwise. \n",
        "\n",
        "![alt text](https://i.imgur.com/R67CHbV.png)\n",
        "\n",
        "Alright now we turn the hidden layers (because theres more than one desipite our viewpoint.) and we make that blue and add a looping arrow. \n",
        "\n",
        "![alt text](https://i.imgur.com/0EyH086.png)\n",
        "\n",
        "This is the temporal loop. not only do we get an output but the loop feeds back into itself to learn.\n",
        "\n",
        "now we unroll this temporal loop (we haven't changed perspective yet) \n",
        "\n",
        "![alt text](https://i.imgur.com/zhwvva3.png)\n",
        "\n",
        "NOW we change our perspective so we can see the layers. \n",
        "\n",
        "![alt text](https://i.imgur.com/ppAboid.png)\n",
        "\n",
        "They are still there we just arent focusing on them. \n",
        "\n",
        "the neurons are connecting themselves through time to remember what they did prevously, similar to our weights. \n",
        "\n",
        "So now lets look at a few examples:\n",
        "\n",
        "**One to many:** One input, but multiple outputs. in this example we have an image and the computer must come up with words to describe the image. We have the image go through a CNN and RNN and the computer will come up with words to describe it. \n",
        "\n",
        "![alt text](https://i.imgur.com/lSNKsCQ.png)\n",
        "\n",
        "\n",
        "**Many to one** - Multiple inputs one output. In this example if a statement is positive or negative. \n",
        "\n",
        "![alt text](https://i.imgur.com/SEJ8og1.png)\n",
        "\n",
        "**Many to Many** - Many inputs many outputs. Google Translate is a perfect example. \n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/3IN5N4a.png)\n",
        "\n",
        "Or adding subtitles to a movie\n",
        "\n",
        "![alt text](https://i.imgur.com/MyWd8vP.png)\n",
        "\n",
        "\n",
        "Additional WATCHING:\n",
        "\n",
        "Sunspring (movie, 2016)\n",
        "\n",
        "Directed by Oscar Sharp \n",
        "\n",
        "Wirrten By Benjamin (AI)\n",
        "\n",
        "https://www.youtube.com/watch?v=LY7x2Ihqjmc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ylXuCVzTh6r",
        "colab_type": "text"
      },
      "source": [
        "# Vanishing Gradient\n",
        "\n",
        "* Discovered by Sepp Hochreiter This math god right here\n",
        "\n",
        "![alt text](https://i.imgur.com/WMdDyll.png)\n",
        "\n",
        "Alongside Yoshua Bengio who suspiciously looks like a time lord and over 500 research papers in his name. He better have his own parking space. \n",
        "\n",
        "![alt text](https://i.imgur.com/zmaxA6y.png)\n",
        "\n",
        "So here is a visual of our vanishing gradient problem. \n",
        "\n",
        "![alt text](https://i.imgur.com/l88cV1i.png)\n",
        "\n",
        "So when we are praciting gradient descent, putting it in laymans terms, with the power of math we are rolling a ball in a halfpipe. said ball leaves us a marker on the map rolling back and forth down the halfpipe until it loses momentum and meets in theroy the lowest point in the pit. \n",
        "\n",
        "![alt text](https://i.imgur.com/VW8uWhv.png)\n",
        "\n",
        "Also remember in the typical convolutional neural network the weights provide us long term memory to learn from not short term.\n",
        "\n",
        "In contrast our RNN is traveling in time and information from previous time points keeps coming through the network.\n",
        "\n",
        "![alt text](https://i.imgur.com/54OIRSk.png)\n",
        "\n",
        "Don't forget there are layers underneth these we cant see from this illustrations perspective. Try to think both 3rd and 4th dimensionally. and in each point in time we can calculate the cost function. \n",
        "\n",
        "![alt text](https://i.imgur.com/0pVFvCQ.png)\n",
        "\n",
        "Lets just focus on the second to last epsilon for this part. So we need to calculate the cost and update the weight to better calculate the output and minimize the error.  \n",
        "\n",
        "![alt text](https://i.imgur.com/PF18scJ.png)\n",
        "\n",
        "THIS IS WHERE WE RUN INTO THE HUGE PROBLEM. We have to update trough time to update the recurring weight. This is used to connect the hidden layers to each other through the time changes when unrolled and we have to multiply this weight multiple times. \n",
        "\n",
        "![alt text](https://i.imgur.com/mBPfEgD.png)\n",
        "\n",
        "We can also tell our network how far back we want to look but the further back we look, the more math needs to be done, and heres where it gets bad for us. All these things we are multiplying are way less than one and this lowers our overall numbers down in the equation on the right which is not something we want to happen bacause it lessens the gradient by a lot as you can see by the green arrows.  \n",
        "\n",
        "![alt text](https://i.imgur.com/K6rfZHx.png)\n",
        "\n",
        "Okay so we are losing gradient. why's that a big deal? Because that's a crucial variable to update our weight. And the weight is what helps our model remember long term. So we're now operating on binge drinking level memory. in short...\n",
        "\n",
        "![alt text](https://i.imgur.com/5DPG1O6.png)\n",
        "\n",
        "Small weights leads to little to no change \n",
        "\n",
        "large weights and we get burdened by bigger numbers to calculate. \n",
        "\n",
        "So what do we do? \n",
        "\n",
        "\n",
        "## Solutions:\n",
        "\n",
        "### 1 Exploding Gradient\n",
        "  * Truncated BackPropagation (Stop updating weights after a certain point.)\n",
        "  \n",
        "  * Penalties (artificially reduce the gradient)\n",
        "  \n",
        "  * Gradient clipping (max limit for the graident)\n",
        "\n",
        "### 2 Vanishing Gradient\n",
        "\n",
        "  * Weight initialization - being smart about how you initialize your weights to minimize the potential for vanish\n",
        "  \n",
        "  * Echo State Networks - Expensive hardware solution to a software problem. \n",
        "  \n",
        "  * Long Short-Term Memory Netowrks (LSTMs) - the go to solution for a happy medium which we will learn more about next. \n",
        "\n",
        "\n",
        "\n",
        "Additional Reading:\n",
        "\n",
        "Untersuchungen Zu Dynamischen Neuronalen Netzen (understanding the dynamic neural network)\n",
        "\n",
        "By Sepp (Josef) Hochreiter (1991)\n",
        "\n",
        "http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf\n",
        "\n",
        "Hope you speak German, but if you don't you can read the math. that's still universal.  \n",
        "\n",
        "\n",
        "Learning Long-Term Depndencies with Gradient Descent is Difficult\n",
        "\n",
        "by Yoshua Bengio et al. (1994)\n",
        "\n",
        "http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf\n",
        "\n",
        "and finally\n",
        "\n",
        "On the Difficulty of training recurrent neural networks \n",
        "\n",
        "by Razvan Pascanu et al. (2013) \n",
        "\n",
        "http://proceedings.mlr.press/v28/pascanu13.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFFLsT-4hlFi",
        "colab_type": "text"
      },
      "source": [
        "# LSTMs\n",
        "\n",
        "* Brief history\n",
        "* LSTM architechure\n",
        "* Example Walkthrough\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/poOY6pZ.png)\n",
        "\n",
        "As we said last time the rec weights applied the smaller our graident becomes which reduces the effectiveness of our weights changes which ultimately effects our long term memory. So what do we do to fix this? \n",
        "\n",
        "We make rek weight = 1\n",
        "\n",
        "We can thank Sepp Hochreiter and Juergen Schmidhuber (Sepp's Supervisior) for this solution. So yeah a parking space and a park bench with their names on it are certainly something they should have earned from this. \n",
        "\n",
        "![alt text](https://i.imgur.com/DguSwxQ.png)\n",
        "\n",
        "So this is what our recurrent neural network looks like when dig inside it\n",
        "\n",
        "![alt text](https://i.imgur.com/H2GwQke.png)\n",
        "\n",
        "we're focusing mainly on the middle part of it. This is what standard RNN looks like before LSTM. Now lets take a look at it when LSTM is applied. \n",
        "\n",
        "![alt text](https://i.imgur.com/ampv2b5.png)\n",
        "\n",
        "Better explain this now.  Lemme thorw another image in to clean things up. \n",
        "\n",
        "![alt text](https://i.imgur.com/Nb1naNM.png)\n",
        "\n",
        "Okay fine we'll go back to the simpler image. The top line with the X and the + is where Weighted rec equals 1. Not much is happening there that gets in the way. It's smooth sailing for our values to pass thorugh.  Some things may be added or removed to it along the way, but the \"time travel\" is otherwise a quick and safe journey for our valuable data. Now lets ignore the left and right cells to get a closer look. \n",
        "\n",
        "![alt text](https://i.imgur.com/eRulmBm.png)\n",
        "\n",
        "Better appreciate that image i had to clean it up in mspaint. C stands for memory, h is outputs and x is the inputs with t of course representing time. \n",
        "\n",
        "An important thing to understand is everything we are seeing here is a vector. there are lots of values of course behind these symbols as usual. \n",
        "\n",
        "so as the vector transfers the outputs and inputs concatenate into one parallel line feeding into the neural network layer operations (the sigmmas and Tanh along the way). \n",
        "\n",
        "Then we have copy which is copied to C and the outputs.\n",
        "\n",
        "![alt text](https://i.imgur.com/2wmkkuQ.png)\n",
        "\n",
        "The pink dots which are pointwise operations these are usually the Fs Vs and Os in math formulas in this case think of them like plumbing valves that you turn to open and close. \n",
        "\n",
        "The leftmost X pointwise operation on the C line is controlled by that sigma bellow it. Sigma determines if it opens or closes. Allowing memory to flow freely or stop. the next sigma working with tanh x  will determine if it is added to or not. Finally we have the one on the right known as the output valve more on that later.\n",
        "\n",
        "![alt text](https://i.imgur.com/q6IKbr3.png)\n",
        "\n",
        "Right here is best to think of this part as a T connected pipe which can add more data for C. \n",
        "\n",
        "![alt text](https://i.imgur.com/bz3HbcR.png)\n",
        "\n",
        "These guys represent parts of our neural network layer ultimately. So that's the varibles controlling these things. \n",
        "\n",
        "\n",
        "### Step By step\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/xxi8Q6W.png)\n",
        "\n",
        "X the new input value comes in along with H (the previous output) and together they are combined with sgma over here to determine if the \"valve\" is opened or closed.\n",
        "\n",
        "![alt text](https://i.imgur.com/0GgnJVG.png)\n",
        "\n",
        "Moving along a lot more layers are going through and out buddy sigmas then decides if those values are going to go through and will be added to the C-line. \n",
        "\n",
        "![alt text](https://i.imgur.com/vhYBlyH.png)\n",
        "\n",
        "The C line since the W rec =1 can take anything else thrown at it relatively easy just dealing with forget or remember whatever.\n",
        "\n",
        "![alt text](https://i.imgur.com/WxDo8Xv.png)\n",
        "\n",
        "finally we reach the last sigma which is our output valve which decides if out memory line can share it's data with the output apply what it's learned. \n",
        "\n",
        "![alt text](https://i.imgur.com/hqaITY8.png)\n",
        "\n",
        "real world example. We're going to change the sentence from boy to girl. \n",
        "\n",
        "![alt text](https://i.imgur.com/TLV2H00.png) \n",
        "\n",
        "So right now the Czech word for boy is on the c line. it's remembering this key word, nothing have changed no \"valves\" opened.\n",
        "\n",
        "X would be when we replace boy with girl X and ht-1 meet up and do math to determine yep this has changed time to update the C line removing the one word, add the replacement word, and finally since C is updated they the current memory might be enough to change the output h. For example the next sentence might have to adjust future sentences with feminine verbs (I don't speak Czech don't know for sure but the concept still applies for our neural network).  \n",
        "\n",
        "\n",
        "Additional Reading:\n",
        "\n",
        "Long Short-Term Memory By Sepp Hochreiter & Jurgen Schmidhuber (1997)\n",
        "\n",
        "https://www.bioinf.jku.at/publications/older/2604.pdf\n",
        "\n",
        "Don't worry this one's in english this time. \n",
        "\n",
        "Understanding LSTM Networks by Christopher Olah (2015)\n",
        "\n",
        "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "Understanding LSTM and it's diagrams\n",
        "\n",
        "by Shi Yan (2016)\n",
        "\n",
        "https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714\n",
        "\n",
        "Imma leave her diagram up in this because it's Honestly more clearly detailed than the one I was taking notes with. \n",
        "\n",
        "![alt text](https://miro.medium.com/max/700/1*laH0_xXEkFE0lKJu54gkFQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQebjcr__Qde",
        "colab_type": "text"
      },
      "source": [
        "# LSTM Practical Intuition\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n"
      ]
    }
  ]
}