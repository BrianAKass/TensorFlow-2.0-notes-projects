{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16-01 recurrent neural networks notes.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wejN71MWxWkw",
        "colab_type": "text"
      },
      "source": [
        "# What we will learn in this:\n",
        "  \n",
        "  * The idea behind Recurrent Neural Networks\n",
        "  \n",
        "  * The Vanishing Gradient Problem\n",
        "  \n",
        "  * Long Short-Term Memory (LSTM) \n",
        "  \n",
        "  * Practical Intuition\n",
        "  \n",
        "  * Extra :  LSTM Variations\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tWcPHBp0ajC",
        "colab_type": "text"
      },
      "source": [
        "# What are Recurrent Neural Networks?\n",
        "\n",
        "![alt text](https://i.imgur.com/3hEbnzI.png)\n",
        "\n",
        "Let's look at the Human Brain:\n",
        "\n",
        "![alt text](https://i.imgur.com/8zdtJ2w.png)\n",
        "\n",
        "The brain despite all it's flaws is still the best frame of reference we use to create our Neural Networks... for now.  So Best to show where we are getting our models from. \n",
        "\n",
        "**Cerebrum** -top/ large area of brain\n",
        "**Cerebellum** - bottom smaller area of the brain\n",
        "**Brainstem** - Glorified puppet stings and feelers which remind me i have chronic back pain on a second by second basis. \n",
        "**Frontal Lobe** - Short term memory. Which is where we have our inspiration for the RNNs \n",
        "**Parietal Lobe**\n",
        "**Occipital Lobe** - Images and objects. so our CNNs are mimicing this area.\n",
        "**Temporal Lobe** - Long term memory. This is how our weights work. They learn from the past and the weights continue being adjusted with each front to back propegation. \n",
        "\n",
        "\n",
        "Okay gonig back to our convolutional neural network.\n",
        "\n",
        "![alt text](https://i.imgur.com/xu87ADA.png)\n",
        "\n",
        "How do we turn this into a Recurrent Neural Network?\n",
        "\n",
        "Squash it.\n",
        "\n",
        "![alt text](https://i.imgur.com/juUGDkb.png)\n",
        "\n",
        "Everything in that model before is still there but try to imagine it as three dimensional. and inseat of looking it from the perspecitve of the previous picture, we are looking at the model from a different angle. \n",
        "\n",
        "![alt text](https://i.imgur.com/htYvHAe.png)\n",
        "\n",
        "everyone thinking third dimensionally now? Cool. Everything's still there but it's all flattened. now line up all our lines and rotate 90 degrees counter clockwise. \n",
        "\n",
        "![alt text](https://i.imgur.com/R67CHbV.png)\n",
        "\n",
        "Alright now we turn the hidden layers (because theres more than one desipite our viewpoint.) and we make that blue and add a looping arrow. \n",
        "\n",
        "![alt text](https://i.imgur.com/0EyH086.png)\n",
        "\n",
        "This is the temporal loop. not only do we get an output but the loop feeds back into itself to learn.\n",
        "\n",
        "now we unroll this temporal loop (we haven't changed perspective yet) \n",
        "\n",
        "![alt text](https://i.imgur.com/zhwvva3.png)\n",
        "\n",
        "NOW we change our perspective so we can see the layers. \n",
        "\n",
        "![alt text](https://i.imgur.com/ppAboid.png)\n",
        "\n",
        "They are still there we just arent focusing on them. \n",
        "\n",
        "the neurons are connecting themselves through time to remember what they did prevously, similar to our weights. \n",
        "\n",
        "So now lets look at a few examples:\n",
        "\n",
        "**One to many:** One input, but multiple outputs. in this example we have an image and the computer must come up with words to describe the image. We have the image go through a CNN and RNN and the computer will come up with words to describe it. \n",
        "\n",
        "![alt text](https://i.imgur.com/lSNKsCQ.png)\n",
        "\n",
        "\n",
        "**Many to one** - Multiple inputs one output. In this example if a statement is positive or negative. \n",
        "\n",
        "![alt text](https://i.imgur.com/SEJ8og1.png)\n",
        "\n",
        "**Many to Many** - Many inputs many outputs. Google Translate is a perfect example. \n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/3IN5N4a.png)\n",
        "\n",
        "Or adding subtitles to a movie\n",
        "\n",
        "![alt text](https://i.imgur.com/MyWd8vP.png)\n",
        "\n",
        "\n",
        "Additional WATCHING:\n",
        "\n",
        "Sunspring (movie, 2016)\n",
        "\n",
        "Directed by Oscar Sharp \n",
        "\n",
        "Wirrten By Benjamin (AI)\n",
        "\n",
        "https://www.youtube.com/watch?v=LY7x2Ihqjmc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ylXuCVzTh6r",
        "colab_type": "text"
      },
      "source": [
        "# Vanishing Gradient\n",
        "\n",
        "* Discovered by Sepp Hochreiter This math god right here\n",
        "\n",
        "![alt text](https://i.imgur.com/WMdDyll.png)\n",
        "\n",
        "Alongside Yoshua Bengio who suspiciously looks like a time lord and over 500 research papers in his name. He better have his own parking space. \n",
        "\n",
        "![alt text](https://i.imgur.com/zmaxA6y.png)\n",
        "\n",
        "So here is a visual of our vanishing gradient problem. \n",
        "\n",
        "![alt text](https://i.imgur.com/l88cV1i.png)\n",
        "\n",
        "So when we are praciting gradient descent, putting it in laymans terms, with the power of math we are rolling a ball in a halfpipe. said ball leaves us a marker on the map rolling back and forth down the halfpipe until it loses momentum and meets in theroy the lowest point in the pit. \n",
        "\n",
        "![alt text](https://i.imgur.com/VW8uWhv.png)\n",
        "\n",
        "Also remember in the typical convolutional neural network the weights provide us long term memory to learn from not short term.\n",
        "\n",
        "In contrast our RNN is traveling in time and information from previous time points keeps coming through the network.\n",
        "\n",
        "![alt text](https://i.imgur.com/54OIRSk.png)\n",
        "\n",
        "Don't forget there are layers underneth these we cant see from this illustrations perspective. Try to think both 3rd and 4th dimensionally. and in each point in time we can calculate the cost function. \n",
        "\n",
        "![alt text](https://i.imgur.com/0pVFvCQ.png)\n",
        "\n",
        "Lets just focus on the second to last epsilon for this part. So we need to calculate the cost and update the weight to better calculate the output and minimize the error.  \n",
        "\n",
        "![alt text](https://i.imgur.com/PF18scJ.png)\n",
        "\n",
        "THIS IS WHERE WE RUN INTO THE HUGE PROBLEM. We have to update trough time to update the recurring weight. This is used to connect the hidden layers to each other through the time changes when unrolled and we have to multiply this weight multiple times. \n",
        "\n",
        "![alt text](https://i.imgur.com/mBPfEgD.png)\n",
        "\n",
        "We can also tell our network how far back we want to look but the further back we look, the more math needs to be done, and heres where it gets bad for us. All these things we are multiplying are way less than one and this lowers our overall numbers down in the equation on the right which is not something we want to happen bacause it lessens the gradient by a lot as you can see by the green arrows.  \n",
        "\n",
        "![alt text](https://i.imgur.com/K6rfZHx.png)\n",
        "\n",
        "Okay so we are losing gradient. why's that a big deal? Because that's a crucial variable to update our weight. And the weight is what helps our model remember long term. So we're now operating on binge drinking level memory. in short...\n",
        "\n",
        "![alt text](https://i.imgur.com/5DPG1O6.png)\n",
        "\n",
        "Small weights leads to little to no change \n",
        "\n",
        "large weights and we get burdened by bigger numbers to calculate. \n",
        "\n",
        "So what do we do? \n",
        "\n",
        "\n",
        "## Solutions:\n",
        "\n",
        "### 1 Exploding Gradient\n",
        "  * Truncated BackPropagation (Stop updating weights after a certain point.)\n",
        "  \n",
        "  * Penalties (artificially reduce the gradient)\n",
        "  \n",
        "  * Gradient clipping (max limit for the graident)\n",
        "\n",
        "### 2 Vanishing Gradient\n",
        "\n",
        "  * Weight initialization - being smart about how you initialize your weights to minimize the potential for vanish\n",
        "  \n",
        "  * Echo State Networks - Expensive hardware solution to a software problem. \n",
        "  \n",
        "  * Long Short-Term Memory Netowrks (LSTMs) - the go to solution for a happy medium which we will learn more about next. \n",
        "\n",
        "\n",
        "\n",
        "Additional Reading:\n",
        "\n",
        "Untersuchungen Zu Dynamischen Neuronalen Netzen (understanding the dynamic neural network)\n",
        "\n",
        "By Sepp (Josef) Hochreiter (1991)\n",
        "\n",
        "http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf\n",
        "\n",
        "Hope you speak German, but if you don't you can read the math. that's still universal.  \n",
        "\n",
        "\n",
        "Learning Long-Term Depndencies with Gradient Descent is Difficult\n",
        "\n",
        "by Yoshua Bengio et al. (1994)\n",
        "\n",
        "http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf\n",
        "\n",
        "and finally\n",
        "\n",
        "On the Difficulty of training recurrent neural networks \n",
        "\n",
        "by Razvan Pascanu et al. (2013) \n",
        "\n",
        "http://proceedings.mlr.press/v28/pascanu13.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFFLsT-4hlFi",
        "colab_type": "text"
      },
      "source": [
        "# LSTMs\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n"
      ]
    }
  ]
}