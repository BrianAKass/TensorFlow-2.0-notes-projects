{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16-01 recurrent neural networks notes.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wejN71MWxWkw",
        "colab_type": "text"
      },
      "source": [
        "# What we will learn in this:\n",
        "  \n",
        "  * The idea behind Recurrent Neural Networks\n",
        "  \n",
        "  * The Vanishing Gradient Problem\n",
        "  \n",
        "  * Long Short-Term Memory (LSTM) \n",
        "  \n",
        "  * Practical Intuition\n",
        "  \n",
        "  * Extra :  LSTM Variations\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tWcPHBp0ajC",
        "colab_type": "text"
      },
      "source": [
        "# What are Recurrent Neural Networks?\n",
        "\n",
        "![alt text](https://i.imgur.com/3hEbnzI.png)\n",
        "\n",
        "Let's look at the Human Brain:\n",
        "\n",
        "![alt text](https://i.imgur.com/8zdtJ2w.png)\n",
        "\n",
        "The brain despite all it's flaws is still the best frame of reference we use to create our Neural Networks... for now.  So Best to show where we are getting our models from. \n",
        "\n",
        "**Cerebrum** -top/ large area of brain\n",
        "**Cerebellum** - bottom smaller area of the brain\n",
        "**Brainstem** - Glorified puppet stings and feelers which remind me i have chronic back pain on a second by second basis. \n",
        "**Frontal Lobe** - Short term memory. Which is where we have our inspiration for the RNNs \n",
        "**Parietal Lobe**\n",
        "**Occipital Lobe** - Images and objects. so our CNNs are mimicing this area.\n",
        "**Temporal Lobe** - Long term memory. This is how our weights work. They learn from the past and the weights continue being adjusted with each front to back propegation. \n",
        "\n",
        "\n",
        "Okay gonig back to our convolutional neural network.\n",
        "\n",
        "![alt text](https://i.imgur.com/xu87ADA.png)\n",
        "\n",
        "How do we turn this into a Recurrent Neural Network?\n",
        "\n",
        "Squash it.\n",
        "\n",
        "![alt text](https://i.imgur.com/juUGDkb.png)\n",
        "\n",
        "Everything in that model before is still there but try to imagine it as three dimensional. and inseat of looking it from the perspecitve of the previous picture, we are looking at the model from a different angle. \n",
        "\n",
        "![alt text](https://i.imgur.com/htYvHAe.png)\n",
        "\n",
        "everyone thinking third dimensionally now? Cool. Everything's still there but it's all flattened. now line up all our lines and rotate 90 degrees counter clockwise. \n",
        "\n",
        "![alt text](https://i.imgur.com/R67CHbV.png)\n",
        "\n",
        "Alright now we turn the hidden layers (because theres more than one desipite our viewpoint.) and we make that blue and add a looping arrow. \n",
        "\n",
        "![alt text](https://i.imgur.com/0EyH086.png)\n",
        "\n",
        "This is the temporal loop. not only do we get an output but the loop feeds back into itself to learn.\n",
        "\n",
        "now we unroll this temporal loop (we haven't changed perspective yet) \n",
        "\n",
        "![alt text](https://i.imgur.com/zhwvva3.png)\n",
        "\n",
        "NOW we change our perspective so we can see the layers. \n",
        "\n",
        "![alt text](https://i.imgur.com/ppAboid.png)\n",
        "\n",
        "They are still there we just arent focusing on them. \n",
        "\n",
        "the neurons are connecting themselves through time to remember what they did prevously, similar to our weights. \n",
        "\n",
        "So now lets look at a few examples:\n",
        "\n",
        "**One to many:** One input, but multiple outputs. in this example we have an image and the computer must come up with words to describe the image. We have the image go through a CNN and RNN and the computer will come up with words to describe it. \n",
        "\n",
        "![alt text](https://i.imgur.com/lSNKsCQ.png)\n",
        "\n",
        "\n",
        "**Many to one** - Multiple inputs one output. In this example if a statement is positive or negative. \n",
        "\n",
        "![alt text](https://i.imgur.com/SEJ8og1.png)\n",
        "\n",
        "**Many to Many** - Many inputs many outputs. Google Translate is a perfect example. \n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/3IN5N4a.png)\n",
        "\n",
        "Or adding subtitles to a movie\n",
        "\n",
        "![alt text](https://i.imgur.com/MyWd8vP.png)\n",
        "\n",
        "\n",
        "Additional WATCHING:\n",
        "\n",
        "Sunspring (movie, 2016)\n",
        "\n",
        "Directed by Oscar Sharp \n",
        "\n",
        "Wirrten By Benjamin (AI)\n",
        "\n",
        "https://www.youtube.com/watch?v=LY7x2Ihqjmc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ylXuCVzTh6r",
        "colab_type": "text"
      },
      "source": [
        "# Vanishing Gradient\n",
        "\n",
        "\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()\n",
        "\n",
        "![alt text]()"
      ]
    }
  ]
}