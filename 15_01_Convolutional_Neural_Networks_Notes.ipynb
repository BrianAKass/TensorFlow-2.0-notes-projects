{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15-01 Convolutional Neural Networks Notes.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfuzbZo81ca5",
        "colab_type": "text"
      },
      "source": [
        "# What are Convolutional Neural Networks?\n",
        "\n",
        "What do you see? is the person looking front or right?\n",
        "\n",
        "![alt text](https://i.imgur.com/VwafbE2.png)\n",
        "\n",
        "Images like that tell us how our brain does it's best to interpret two conflicting features based on what we know in our realities. You Have seen many optical illusions that work along this principle. Ultimately your brain is doing the best it can to sort the visual information given based on real world interactions. But images are not limited by these things and can be distorted to create images that challenge these fundumenatal things we have instinctually applied in our lives. \n",
        "\n",
        "This is the underlining challenge we give Convolutional Neural Networks.  Here are some examples we give in a test set. \n",
        "\n",
        "![alt text](https://i.imgur.com/oQ4DEUm.png)\n",
        "\n",
        "And now here is how it determines the images. \n",
        "\n",
        "![alt text](https://i.imgur.com/88lcWmb.png)\n",
        "\n",
        "on the last image, it became diffficult for the neural network to determine a more clear cut answer. \n",
        "\n",
        "This remakable technology has gained popularity in recent years due to it's approach or problem solving. Outstaging artifical neural networks. \n",
        "\n",
        "So how do they work? \n",
        "\n",
        "Lets start with the input image, CNN, and the output label aka Image class.\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/Via2QEH.png)\n",
        "\n",
        "And in said images we are trying to figure out if somone is happy or sad. So how would a Neural network determine this?\n",
        "\n",
        "Lets say we have two images one is black and white and the other is in color. both are 2x2px the black an white image is a 2d array to the CNN because remember the Neural netowrk looks at values between 0 and 255 and since we are only seeing in black and white it's 2 dimensional. So in contrast the colored image is seen as 3 dimensional. classic rgb layers and what not\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/TWVxkQ7.png)\n",
        "\n",
        "\n",
        "so when we look at the image of a happy person or a sad person its really checking the values of the image. This is why we typically remove the 255 values when we are training images.\n",
        "\n",
        "![alt text](https://i.imgur.com/ilaTPuw.png)\n",
        "\n",
        "\n",
        "\n",
        "Additional reading \n",
        "\n",
        "Gradient Based Learning Applied to Document Recognition\n",
        "\n",
        "By Yann LeCun et al. (1998)\n",
        "\n",
        "Link:\n",
        "\n",
        "http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXEyNAGp1chj",
        "colab_type": "text"
      },
      "source": [
        "# Convolution\n",
        "\n",
        "Here is the convolution function\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/O32Jy2K.png)\n",
        "\n",
        "If you want to get good at the maths this read might help you.\n",
        "\n",
        "Introduction to Convolutional Neural Networks by Jianxin Wu\n",
        "\n",
        "https://cs.nju.edu.cn/wujx/teaching/15_CNN.pdf\n",
        "\n",
        "So what is convolution in intuitive terms?\n",
        "\n",
        "Lets look at our input image of the smile again.\n",
        "\n",
        "![alt text](https://i.imgur.com/Q4MLExO.png)\n",
        "\n",
        "next to it is a feature detector the size ranges but mostly 3x3.\n",
        "\n",
        "also called kernel or filter. \n",
        "\n",
        "a convolutional operation is represented by the x circle in the next image. \n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/vWX0NML.png)\n",
        "\n",
        "\n",
        "So we are taking this feature detector and the selection of the input image and we are multiplying each value by each value. from there we add up the results in our feature map... \n",
        "\n",
        "![alt text](https://i.imgur.com/R3eFW6B.png)\n",
        "\n",
        "![alt text](https://i.imgur.com/YDF7XeX.png)\n",
        "\n",
        "\n",
        "we can change the spacing by changing whats called the stride to effect our outcome.. most common stride is 2\n",
        "\n",
        "![alt text](https://i.imgur.com/27KkUbF.png)\n",
        "\n",
        "As we see when we are done some areas match more than others. we do not simply recieve a binary output on our feature map (aka convulved feature or activation map) . Said map reduces the size of the image. we lose some information from this process but we are trying to detect key features not necessarily every aspect of the image. Just like us we dont remember the color of the shirt the person was wearing when they smile. We loop at eyes lips and teeth potentially. \n",
        "\n",
        "![alt text](https://i.imgur.com/WgBk2sb.png)\n",
        "\n",
        "We create multiple feature maps because we are using different filters based on parts of the original image to capute them. \n",
        "\n",
        "Image editors like gimp take advantage of this to effect the image. !\n",
        "\n",
        "[alt text](https://i.imgur.com/Zes3wnA.png)\n",
        "\n",
        "![alt text](https://i.imgur.com/bq4MmkA.png)\n",
        "\n",
        "There are other examples of this that produce radically different results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHPAGNZy1ckX",
        "colab_type": "text"
      },
      "source": [
        "# Bis- ReLU Layer\n",
        "\n",
        "So we have our input image and our convolutional layer. Now we apply our rectifier function\n",
        "\n",
        "![alt text](https://i.imgur.com/UMtCeO3.png)\n",
        "\n",
        "We conisder adding the rectifier as part of the convolutional layer step.\n",
        "\n",
        "We are applying the rectifier because we want to increase non linearity in our convolutional nueral network. The rectifier acts as that filter/function that breaks up the linearity. Images are not linear\n",
        "\n",
        "further reading:\n",
        "\n",
        "Understanding Convolutional Neural Networks with A Mathematical Model\n",
        "\n",
        "By C.-C. Jay Kuo\n",
        "\n",
        "https://arxiv.org/pdf/1609.04112.pdf\n",
        "\n",
        "\n",
        "Delving Deep into Recifiers:\n",
        "Surpassing Human-Level Performance on ImageNet Classification\n",
        "\n",
        "by Kaiming He et al. (2015)\n",
        "\n",
        "https://arxiv.org/pdf/1502.01852.pdf\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-XgCnHp1cm9",
        "colab_type": "text"
      },
      "source": [
        "# Max Pooling\n",
        "\n",
        "![alt text](https://i.imgur.com/8Uvqibj.png)\n",
        "![alt text](https://i.imgur.com/GtBltPD.png)\n",
        "\n",
        "How do we get a Neural network to recognize not just one image of a cheetah, but all cheetahs if they are at different shpaes sizes and locations are different? \n",
        "\n",
        "So we need to have flexibility in the model so it can pick up on a more broader interpretation of what makes a cheetah. we do this by pooling.\n",
        "\n",
        "![alt text](https://i.imgur.com/S1HcxnS.png)\n",
        "\n",
        "Just like how we applied our feature map, we sample that map and find the max value in each section of it. \n",
        "\n",
        "![alt text](https://i.imgur.com/ttJYqnH.png)\n",
        "\n",
        "As you can see we preserved all of our features. more specifically the 2s and 4s in this case. Machines have objectively better memry than us because everything is written down to them. When we simplify the data pertaining only the information relevant we apply to it the necessary information to recognize the cheetah. not an exact single case instance. As a bonus we slim up file sizes!\n",
        "\n",
        "\n",
        "Further reading:\n",
        "\n",
        "Evaluation of Pooling Operations in Convolutional Architechtures for Object Recognition\n",
        "\n",
        "By Dominik Scherer et al. (2010)\n",
        "\n",
        "http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf\n",
        "\n",
        "This ones a lighter read compared to othrs I've listed. \n",
        "\n",
        "\n",
        "Another thing to note is we can apply Mean Pooling and Min pooling as well to adjust our neural network further. \n",
        "\n",
        "So to sum up where we are at right now. \n",
        "\n",
        "![alt text](https://i.imgur.com/J28zJNc.png)\n",
        "\n",
        "http://scs.ryerson.ca/~aharley/vis/conv/ \n",
        "\n",
        "This is a visualization of a CNN You can play with. Quite fascinating stuff. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg2URooN1cpc",
        "colab_type": "text"
      },
      "source": [
        "# Flattening\n",
        "\n",
        "Simple step. Mostly images. \n",
        "\n",
        "![alt text](https://i.imgur.com/3LxTMbF.png)\n",
        "\n",
        "Flatten layers then you get a vector of inputs for the artificial neural network. \n",
        "\n",
        "![alt text](https://i.imgur.com/JxByjNi.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5PsFvdN1csR",
        "colab_type": "text"
      },
      "source": [
        "# Full Connection\n",
        "\n",
        "### Adding an artificial network to our convolutional neural network\n",
        "\n",
        "![alt text](https://i.imgur.com/UqKyQSf.png)\n",
        "\n",
        "The Fully Connected Layer is where the hidden layer is. But the Fully Connected part is the key thing here. That's not standard for a neural network. \n",
        "\n",
        "Heres a bigger version of the same principle at work here:\n",
        "\n",
        "![alt text](https://i.imgur.com/uMSVPne.png)\n",
        "\n",
        "so going through this network we are trying to guess if an image is the cat or dog. \n",
        "\n",
        "Lets say we run through the network and it's an 80% chance it's a dog and turns out it was actually a cat. we run the cost function and get our mean squarred error. once all that stuff is calculated, its back propagated andthe weights are adjusted top optimize the performance. Feature detection is also adjusted in this practice. SO just like a typical artifical neural network but we also adjust the feature detection in this process.\n",
        "\n",
        "![alt text](https://i.imgur.com/76NOuE5.png)\n",
        "\n",
        "So how does the output part work since there are two output neurons? First we needweights calculated for each of them. again it's all about coorelations and relationships to the given data. \n",
        "\n",
        "![alt text](https://i.imgur.com/jqeEKvw.png)\n",
        "\n",
        "In this case the neurons closer to one are more important than the other neurons. \n",
        "\n",
        "the Cat in turn has it's own relations. \n",
        "\n",
        "![alt text](https://i.imgur.com/Bi8gFaU.png)\n",
        "\n",
        "Now they apply what they learn \n",
        "\n",
        "![alt text](https://i.imgur.com/x6aHiWJ.png)\n",
        "\n",
        "\n",
        "So that's How we get those probabilities in our neural network in the beginning of these notes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0bD5tJB1cvD",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "![alt text](https://i.imgur.com/lEN375d.png)\n",
        "\n",
        "Further reading\n",
        "\n",
        "The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)\n",
        "\n",
        "Adit Deshpande (2016)\n",
        "\n",
        "Link \n",
        "\n",
        "https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIqsOfEY1cx0",
        "colab_type": "text"
      },
      "source": [
        "# Softmax & Cross-Entropy\n",
        "\n",
        "![alt text](https://i.imgur.com/ruIz10M.png)\n",
        "\n",
        "So how do these two neurons know that thier percentage plus the other neuron's will total 1? they dont.  The only reason they do is because of the softmax function. \n",
        "\n",
        "![alt text](https://i.imgur.com/15DD1Ak.png)\n",
        "\n",
        "to make a long story short it helps get the two values between 0 and 1\n",
        " \n",
        " ![alt text](https://i.imgur.com/rN23WxP.png)\n",
        " \n",
        " \n",
        " What normally goes hand in hand with Softmax is it's cousin Cross-Entropy which comes in two different forms. \n",
        " \n",
        " ![alt text](https://i.imgur.com/5ClI5zN.png)\n",
        " \n",
        " Lets say we put a dog into the network and prediction value is 0.9 so we have our guess and our actual answer plugged into the equation. \n",
        " \n",
        " ![alt text](https://i.imgur.com/j1hV6fy.png)\n",
        " \n",
        " Now lets say we have run two neural networks and had it check a few different images. Now we see what kind of errors we get. \n",
        " \n",
        " **Clasification Errors** - How many you got right or wrong.\n",
        " \n",
        " **Mean Squared Error** - sum of squarred errors and average them.\n",
        " \n",
        " **Cross-entropy** \n",
        " \n",
        " ![alt text](https://i.imgur.com/bfGZvkt.png)\n",
        " \n",
        " \n",
        " Ultimately Cross entropy can show a more significant level of changes happening that MSE and CE can not show us. for example going from 0.0001 to 0.01\n",
        " \n",
        " Keep in mind this is best suited for classification. \n",
        " \n",
        " ### Additional Reading:\n",
        " \n",
        " A friendly Introduction to Cross Entropy Loss\n",
        " \n",
        " By Rob DiPietro (2016)\n",
        " \n",
        " Link: \n",
        " \n",
        " https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/\n",
        " \n",
        " \n",
        " How to implement a Neural Netowrk Intermezzo 2 \n",
        " \n",
        " By Peter Roelants (2016)\n",
        " \n",
        " Link:\n",
        " \n",
        " https://peterroelants.github.io/posts/cross-entropy-softmax/"
      ]
    }
  ]
}